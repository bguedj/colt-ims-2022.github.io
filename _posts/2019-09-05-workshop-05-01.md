---
title:  "DALI/ELLIS: Theory, Algorithms and Computations of Modern Learning Systems"
layout: multitrack
abstract: "TBA"
organizer_url: http://url3.com
categories:
- dali2019b
organizers:
- given: Francis
  family: Bach
- given: Philipp
  family: Hennig
- given: Lorenzo
  family: Rosasco
room: ""
show_abstracts: true
talks:
- title: "Welcome"
  author:
  - given: Francis
    family: Bach
  abstract: My abstract here
  start: "9:30"
  end: "9:40"
- title: "Closing the gap from kernel to deep models"
  author:
  - given: Ingo
    family: Steinwart
  abstract: My abstract here
  start: "9:40"
  end: "10:10"
- title: "Invited Talk"
  author:
  - given: Tamara
    family: Broderick
  start: "10:10"
  end: "10:40"
- title: "Coffee Break"

- title: "Plenary Discussion I"
  author:
  - given: Mario
    family: Figueireido
  - given: Julien
    family: Mairal
  start: "11:30"
  end: "12:15"
- title: "Plenary Discussion II"
  author:
  - given: Philipp
    family: Hennig
  - given: Thomas
    family: Schön
  abstract: My abstract here
  start: "12:15"
  end: "13:00"

- title: "Lunch Break"

- title: "Invited Talk"
  author:
  - given: Gergely
    family: Neu
  abstract: My abstract here
  start: "16:00"
  end: "16:30"
- title: "Invited Talk"
  author:
  - given: Alessandro
    family: Rudi
  abstract: My abstract here
  start: "16:30"
  end: "17:00"
- title: "Invited Talk"
  author:
  - given: Marco
    family: Cuturi
  abstract: |
    Differentiable Ranking and Sorting using Optimal Transport: We consider in
    this work proxy operators for ranking and sorting that are differentiable.
    To do so, we leverage the +fact that sorting can be seen as a particular
    instance of the optimal transport (OT) problem between two univariate
    uniform probability measures, the first measure being supported on the
    family of input values of interest, the second supported on a family of
    increasing target values (e.g. 1,2,...,n if the input array has n
    elements). Building upon this link, we propose generalized rank and sort
    operators by considering arbitrary target measures supported on m values,
    where m can be smaller than n. We recover differentiable algorithms by
    adding to this generic OT problem an entropic regularization, and
    approximate its outputs using Sinkhorn iterations. We call these operators
    soft-ranks and soft-sorts. We show that they are competitive with the
    recently proposed neuralsort (grover, 2019). To illustrate their
    versatility, we use the soft-rank operator to propose a new classification
    training loss that is a differentiable proxy of the 0/1 loss. Using the
    soft-sort operator, we propose a new differentiable loss for trimmed
    regression.

  start: "17:00"
  end: "17:30"

- title: "Coffee Break"

- title: "Invited Talk"
  author:
  - given: Patrick
    family: Rebeschini
  abstract: |
    Implicit Regularization for Optimal Sparse Recovery: Ridge regression is a
    fundamental paradigm in machine learning and statistics, and it has long
    been known to be closely connected to the implicit regularization
    properties of gradient descent methods, cf. early stopping. Over the past
    decade, this connection has sparked research into a variety of directions
    aimed at developing computationally efficiency estimators, including
    acceleration, mini-batching, averaging, sketching, sub-sampling,
    preconditioning, and decentralization. Sparse recovery is another
    cornerstone of modern statistics and learning frameworks. Yet, perhaps
    surprisingly, here the connection to implicit regularization is not as well
    developed. Most results in the literature only involve limit statements
    (holding at convergence, for infinitesimal step sizes), apply to regimes
    with no (or limited) noise, and do not focus on computational efficiency.
    In this talk, we address the following question: Can we establish an
    implicit regularization theory for gradient descent to yield optimal sparse
    recovery in noisy settings, achieving minimax rates with the same cost of
    reading the data? We will highlight the key ideas to obtain some first
    results here, along with a few promising research directions. (Based on
    joint work with Tomas Vaškevičius and Varun Kanade)

  start: "18:00"
  end: "18:30"
- title: "Invited Talk"
  author:
  - given: Vianney
    family: Perchet
  abstract: My abstract here
  start: "18:30"
  end: "19:00"
- title: "Invited Talk"
  author:
  - given: Jean-Philippe
    family: Vert
  abstract: |
    Post-selection inference for nonlinear feature selection: Model selection
    is an essential task for many applications in scientific discovery. The
    most common approaches rely on univariate linear measures of association
    between each feature and the outcome. Such classical selection procedures
    fail to take into account nonlinear effects and interactions between
    features. Kernel-based selection procedures have been proposed as a
    solution. However, current strategies for kernel selection fail to measure
    the significance of a joint model constructed through the combination of
    the basis kernels. In the present work, we exploit recent advances in
    post-selection inference to propose a valid statistical test for the
    association of a joint model of the selected kernels with the outcome. The
    kernels are selected via a step-wise procedure which we model as a
    succession of quadratic constraints in the outcome variable.

  start: "19:00"
  end: "19:30"
- title: "ELLIS assembly"
  start: "19:45"
  end: "20:30"
- title: "Banquet"
  start: "21:00"
---
